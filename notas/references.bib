@book{Ross2013,
  title = {Simulation},
  author = {Ross, Sheldon M.},
  year = {2013},
  isbn = {5-9518-0156-7},
}
@book{Efron1993,
  title = {An {{Introduction}} to the {{Bootstrap}}},
  author = {Efron, Bradley and Tibshirani, Robert J.},
  year = {1993},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  isbn = {978-0-412-04231-7 978-1-4899-4541-9},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Efron1993 - An Introduction to the Bootstrap.pdf},
}
@book{Robert2013a,
  title = {Monte {{Carlo Statistical Methods}}},
  author = {Robert, Christian and Casella, George},
  year = {2013},
  month = {mar},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Monte Carlo statistical methods, particularly those based on Markov chains, are now an essential component of the standard set of techniques used by statisticians. This new edition has been revised towards a coherent and flowing coverage of these simulation techniques, with incorporation of the most recent developments in the field. In particular, the introductory coverage of random variable generation has been totally revised, with many concepts being unified through a fundamental theorem of simulation There are five completely new chapters that cover Monte Carlo control, reversible jump, slice sampling, sequential Monte Carlo, and perfect sampling. There is a more in-depth coverage of Gibbs sampling, which is now contained in three consecutive chapters. The development of Gibbs sampling starts with slice sampling and its connection with the fundamental theorem of simulation, and builds up to two-stage Gibbs sampling and its theoretical properties. A third chapter covers the multi-stage Gibbs sampler and its variety of applications. Lastly, chapters from the previous edition have been revised towards easier access, with the examples getting more detailed coverage. This textbook is intended for a second year graduate course, but will also be useful to someone who either wants to apply simulation techniques for the resolution of practical problems or wishes to grasp the fundamental principles behind those methods. The authors do not assume familiarity with Monte Carlo techniques (such as random variable generation), with computer programming, or with any Markov chain theory (the necessary concepts are developed in Chapter 6). A solutions manual, which covers approximately 40\% of the problems, is available for instructors who require the book for a course. Christian P. Robert is Professor of Statistics in the Applied Mathematics Department at Universit\'e Paris Dauphine, France. He is also Head of the Statistics Laboratory at the Center for Research in Economics and Statistics (CREST) of the National Institute for Statistics and Economic Studies (INSEE) in Paris, and Adjunct Professor at Ecole Polytechnique. He has written three other books, including The Bayesian Choice, Second Edition, Springer 2001. He also edited Discretization and MCMC Convergence Assessment, Springer 1998. He has served as associate editor for the Annals of Statistics and the Journal of the American Statistical Association. He is a fellow of the Institute of Mathematical Statistics, and a winner of the Young Statistician Award of the Societi\'e de Statistique de Paris in 1995. George Casella is Distinguished Professor and Chair, Department of Statistics, University of Florida. He has served as the Theory and Methods Editor of the Journal of the American Statistical Association and Executive Editor of Statistical Science. He has authored three other textbooks: Statistical Inference, Second Edition, 2001, with Roger L. Berger; Theory of Point Estimation, 1998, with Erich Lehmann; and Variance Components, 1992, with Shayle R. Searle and Charles E. McCulloch. He is a fellow of the Institute of Mathematical Statistics and the American Statistical Association, and an elected fellow of the International Statistical Institute.},
  googlebooks = {3G2vBQAAQBAJ},
  isbn = {978-1-4757-4145-2},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Robert2013 - Monte Carlo Statistical Methods.pdf},
}
@book{Glasserman2013,
  title = {Monte {{Carlo Methods}} in {{Financial Engineering}}},
  author = {Glasserman, Paul},
  year = {2013},
  month = {mar},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Monte Carlo simulation has become an essential tool in the pricing of derivative securities and in risk management. These applications have, in turn, stimulated research into new Monte Carlo methods and renewed interest in some older techniques. This book develops the use of Monte Carlo methods in finance and it also uses simulation as a vehicle for presenting models and ideas from financial engineering. It divides roughly into three parts. The first part develops the fundamentals of Monte Carlo methods, the foundations of derivatives pricing, and the implementation of several of the most important models used in financial engineering. The next part describes techniques for improving simulation accuracy and efficiency. The final third of the book addresses special topics: estimating price sensitivities, valuing American options, and measuring market risk and credit risk in financial portfolios. The most important prerequisite is familiarity with the mathematical tools used to specify and analyze continuous-time models in finance, in particular the key ideas of stochastic calculus. Prior exposure to the basic principles of option pricing is useful but not essential. The book is aimed at graduate students in financial engineering, researchers in Monte Carlo simulation, and practitioners implementing models in industry. Mathematical Reviews, 2004: "... this book is very comprehensive, up-to-date and useful tool for those who are interested in implementing Monte Carlo methods in a financial context."},
  googlebooks = {aeAlBQAAQBAJ},
  isbn = {978-0-387-21617-1},
  langid = {english},
  keywords = {Business \& Economics / Accounting / General,Business \& Economics / Economics / Theory,Business \& Economics / Public Finance,Mathematics / Applied,Mathematics / General,Mathematics / Probability \& Statistics / General},
}
@book{Reich2015,
  title = {Probabilistic Forecasting and {{Bayesian}} Data Assimilation},
  author = {Reich, Sebastian and Cotter, Colin},
  year = {2015},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  isbn = {978-1-107-06939-8 978-1-107-66391-6},
  langid = {english},
  lccn = {QA279.5 .R45 2015},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Reich2015 - Probabilistic forecasting and Bayesian data assimilation.pdf},
}
@book{Sullivan2015,
  title = {Introduction to {{Uncertainty Quantification}}},
  author = {Sullivan, T.J.},
  year = {2015},
  series = {Texts in {{Applied Mathematics}}},
  volume = {63},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-23395-6},
  isbn = {978-3-319-23394-9 978-3-319-23395-6},
  langid = {english},
  file = {/Users/agarbuno/Zotero/storage/HLR6PSRT/Sullivan - 2015 - Introduction to Uncertainty Quantification.pdf;/Volumes/GoogleDrive/My Drive/bibliography/Book/Sullivan2015 - Introduction to Uncertainty Quantification.pdf},
}
@book{Smith2013,
  title = {Uncertainty {{Quantification}}: {{Theory}}, {{Implementation}}, and {{Applications}}},
  author = {Smith, Ralph C},
  year = {2013},
  volume = {12},
  publisher = {{SIAM}},
  keywords = {Computers / Computer Simulation,Mathematics / Applied,Mathematics / Probability \& Statistics / General,Science / Physics / Mathematical \& Computational},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Smith2013 - Uncertainty Quantification2.pdf},
}
@book{Dogucu2021,
  title = {Bayes {{Rules}}! {{An Introduction}} to {{Applied Bayesian Modeling}}},
  author = {Dogucu, Miles Q. Ott, Mine, Alicia A. Johnson},
  year = {2021},
  abstract = {An introduction to applied Bayesian modeling.},
  file = {/Users/agarbuno/Zotero/storage/XLCW9T4W/index.html},
}
@article{Sanz-Alonso2019,
  title = {Inverse {{Problems}} and {{Data Assimilation}}},
  author = {{Sanz-Alonso}, Daniel and Stuart, Andrew M. and Taeb, Armeen},
  year = {2019},
  month = {jul},
  journal = {arXiv:1810.06191 [stat]},
  eprint = {1810.06191},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {These notes are designed with the aim of providing a clear and concise introduction to the subjects of Inverse Problems and Data Assimilation, and their inter-relations, together with citations to some relevant literature in this area. The first half of the notes is dedicated to studying the Bayesian framework for inverse problems. Techniques such as importance sampling and Markov Chain Monte Carlo (MCMC) methods are introduced; these methods have the desirable property that in the limit of an infinite number of samples they reproduce the full posterior distribution. Since it is often computationally intensive to implement these methods, especially in high dimensional problems, approximate techniques such as approximating the posterior by a Dirac or a Gaussian distribution are discussed. The second half of the notes cover data assimilation. This refers to a particular class of inverse problems in which the unknown parameter is the initial condition of a dynamical system, and in the stochastic dynamics case the subsequent states of the system, and the data comprises partial and noisy observations of that (possibly stochastic) dynamical system. We will also demonstrate that methods developed in data assimilation may be employed to study generic inverse problems, by introducing an artificial time to generate a sequence of probability measures interpolating from the prior to the posterior.},
  archiveprefix = {arXiv},
  file = {/Users/agarbuno/Library/CloudStorage/GoogleDrive-alfredogarbuno@gmail.com/My Drive/bibliography/Journal Article/Sanz-Alonso2019 - Inverse Problems and Data Assimilation.pdf;/Users/agarbuno/Zotero/storage/XCRBSU2E/1810.html},
}
@book{Reich2015,
  title = {Probabilistic Forecasting and {{Bayesian}} Data Assimilation},
  author = {Reich, Sebastian and Cotter, Colin},
  year = {2015},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  isbn = {978-1-107-06939-8 978-1-107-66391-6},
  langid = {english},
  lccn = {QA279.5 .R45 2015},
  file = {/Users/agarbuno/Library/CloudStorage/GoogleDrive-alfredogarbuno@gmail.com/My Drive/bibliography/Book/Reich2015 - Probabilistic forecasting and Bayesian data assimilation.pdf},
}
@book{Wickham2019,
  title = {Advanced {{R}}, {{Second Edition}}},
  author = {Wickham, Hadley},
  year = {2019},
  month = {may},
  publisher = {{CRC Press}},
  abstract = {Advanced R helps you understand how R works at a fundamental level. It is designed for R programmers who want to deepen their understanding of the language, and programmers experienced in other languages who want to understand what makes R different and special. This book will teach you the foundations of R; three fundamental programming paradigms (functional, object-oriented, and metaprogramming); and powerful techniques for debugging and optimisingyour code.By reading this book, you will learn: The difference between an object and its name, and why the distinction is important The important vector data structures, how they fit together, and how you can pull them apart using subsetting The fine details of functions and environments The condition system, which powers messages, warnings, and errors The powerful functional programming paradigm, which can replace many for loops The three most important OO systems: S3, S4, and R6 The tidy eval toolkit for metaprogramming, which allows you to manipulate code and control evaluation Effective debugging techniques that you can deploy, regardless of how your code is run How to find and remove performance bottlenecks The second edition is a comprehensive update: New foundational chapters: "Names and values," "Control flow," and "Conditions" comprehensive coverage of object oriented programming with chapters on S3, S4, R6, and how to choose between them Much deeper coverage of metaprogramming, including the new tidy evaluation framework use of new package like rlang (http://rlang.r-lib.org), which provides a clean interface to low-level operations, and purr (http://purrr.tidyverse.org/) for functional programming Use of color in code chunks and figuresHadley Wickham is Chief Scientist at RStudio, an Adjunct Professor at Stanford University and the University of Auckland, and a member of the R Foundation. He is the lead developer of the tidyverse, a collection of R packages, including ggplot2 and dplyr, designed to support data science. He is also the author of R for Data Science (with Garrett Grolemund), R Packages, and ggplot2: Elegant Graphics for Data Analysis.},
  googlebooks = {JAOaDwAAQBAJ},
  isbn = {978-1-351-20129-2},
  langid = {english},
}
@book{Meyn1993,
  title = {Markov {{Chains}} and {{Stochastic Stability}}},
  author = {Meyn, Sean P. and Tweedie, Richard L.},
  year = {1993},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-3267-7},
  isbn = {978-1-4471-3269-1 978-1-4471-3267-7},
  langid = {english},
  file = {/Users/agarbuno/Library/CloudStorage/GoogleDrive-alfredogarbuno@gmail.com/My Drive/bibliography/Book/Meyn1993 - Markov Chains and Stochastic Stability.pdf},
}
@book{Gelman2014a,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year = {2014},
  volume = {2},
  publisher = {{CRC press Boca Raton, FL}},
}
@book{Cressie2015,
  title = {Statistics for Spatio-Temporal Data},
  author = {Cressie, Noel and Wikle, Christopher K},
  year = {2015},
  publisher = {{John Wiley \& Sons}},
}
@book{Liu2004,
  title = {Monte {{Carlo Strategies}} in {{Scientific Computing}}},
  author = {Liu, Jun S.},
  year = {2004},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-76371-2},
  isbn = {978-0-387-76369-9 978-0-387-76371-2},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Liu2004 - Monte Carlo Strategies in Scientific Computing.pdf},
}
@book{Brooks2011,
  title = {Handbook of Markov Chain Monte Carlo},
  author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  year = {2011},
  publisher = {{CRC press}},
}
@book{Kruschke2014,
  title = {Doing {{Bayesian}} Data Analysis: {{A}} Tutorial with {{R}}, {{JAGS}}, and {{Stan}}},
  author = {Kruschke, John},
  year = {2014},
  publisher = {{Academic Press}},
}
@inproceedings{Hornik2003,
  title = {{{JAGS}}: {{A}} Program for Analysis of {{Bayesian}} Graphical Models Using {{Gibbs}} Sampling},
  booktitle = {Proceedings of {{DSC}}},
  author = {Hornik, Kurt and Leisch, Friedrich and Zeileis, Achim},
  year = {2003},
  volume = {2},
  pages = {1--1},
}
@article{Salvatier2016,
  title = {Probabilistic Programming in {{Python}} Using {{PyMC3}}},
  author = {Salvatier, John and Wiecki, Thomas V and Fonnesbeck, Christopher},
  year = {2016},
  journal = {PeerJ Computer Science},
  volume = {2},
  pages = {e55},
}
@article{Christen2010,
  title = {A General Purpose Sampling Algorithm for Continuous Distributions (the t-Walk)},
  author = {Christen, J. Andr{\'e}s and Fox, Colin},
  year = {2010},
  month = {jun},
  journal = {Bayesian Analysis},
  volume = {5},
  number = {2},
  pages = {263--281},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/10-BA603},
  abstract = {We develop a new general purpose MCMC sampler for arbitrary continuous distributions that requires no tuning. We call this MCMC the t-walk. The t-walk maintains two independent points in the sample space, and all moves are based on proposals that are then accepted with a standard Metropolis-Hastings acceptance probability on the product space. Hence the t-walk is provably convergent under the usual mild requirements. We restrict proposal distributions, or `moves', to those that produce an algorithm that is invariant to scale, and approximately invariant to affine transformations of the state space. Hence scaling of proposals, and effectively also coordinate transformations, that might be used to increase efficiency of the sampler, are not needed since the t-walk's operation is identical on any scaled version of the target distribution. Four moves are given that result in an effective sampling algorithm. We use the simple device of updating only a random subset of coordinates at each step to allow application of the t-walk to high-dimensional problems. In a series of test problems across dimensions we find that the t-walk is only a small factor less efficient than optimally tuned algorithms, but significantly outperforms general random-walk M-H samplers that are not tuned for specific problems. Further, the t-walk remains effective for target distributions for which no optimal affine transformation exists such as those where correlation structure is very different in differing regions of state space. Several examples are presented showing good mixing and convergence characteristics, varying in dimensions from 1 to 200 and with radically different scale and correlation structure, using exactly the same sampler. The t-walk is available for R, Python, MatLab and C++ at http://www.cimat.mx/\textasciitilde jac/twalk/},
  keywords = {Bayesian inference,Database Expansion Item,MCMC,simulation,t-walk},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Christen2010 - A general purpose sampling algorithm for continuous distributions (the t-walk).pdf;/Users/agarbuno/Zotero/storage/UHLU7UHP/10-BA603.html},
}
@article{Foreman-Mackey2013,
  title = {Emcee: {{The MCMC Hammer}}},
  shorttitle = {Emcee},
  author = {{Foreman-Mackey}, Daniel and Hogg, David W. and Lang, Dustin and Goodman, Jonathan},
  year = {2013},
  month = {mar},
  journal = {Publications of the Astronomical Society of the Pacific},
  volume = {125},
  number = {925},
  eprint = {1202.3665},
  eprinttype = {arxiv},
  pages = {306--312},
  issn = {00046280, 15383873},
  doi = {10.1086/670067},
  abstract = {We introduce a stable, well tested Python implementation of the affine-invariant ensemble sampler for Markov chain Monte Carlo (MCMC) proposed by Goodman \& Weare (2010). The code is open source and has already been used in several published projects in the astrophysics literature. The algorithm behind emcee has several advantages over traditional MCMC sampling methods and it has excellent performance as measured by the autocorrelation time (or function calls per independent sample). One major advantage of the algorithm is that it requires hand-tuning of only 1 or 2 parameters compared to \$\textbackslash sim N\^2\$ for a traditional algorithm in an N-dimensional parameter space. In this document, we describe the algorithm and the details of our implementation and API. Exploiting the parallelism of the ensemble method, emcee permits any user to take advantage of multiple CPU cores without extra effort. The code is available online at http://dan.iel.fm/emcee under the MIT License.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Physics - Computational Physics,Statistics - Computation},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Foreman-Mackey2013 - emcee.pdf;/Users/agarbuno/Zotero/storage/IG557KL7/1202.html},
}
@article{Garbuno-Inigo2019,
  title = {Interacting {{Langevin Diffusions}}: {{Gradient Structure And Ensemble Kalman Sampler}}},
  shorttitle = {Interacting {{Langevin Diffusions}}},
  author = {{Garbuno-Inigo}, Alfredo and Hoffmann, Franca and Li, Wuchen and Stuart, Andrew M.},
  year = {2019},
  month = {oct},
  journal = {arXiv:1903.08866 [math]},
  eprint = {1903.08866},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {Solving inverse problems without the use of derivatives or adjoints of the forward model is highly desirable in many applications arising in science and engineering. In this paper we propose a new version of such a methodology, a framework for its analysis, and numerical evidence of the practicality of the method proposed. Our starting point is an ensemble of over-damped Langevin diffusions which interact through a single preconditioner computed as the empirical ensemble covariance. We demonstrate that the nonlinear Fokker-Planck equation arising from the mean-field limit of the associated stochastic differential equation (SDE) has a novel gradient flow structure, built on the Wasserstein metric and the covariance matrix of the noisy flow. Using this structure, we investigate large time properties of the Fokker-Planck equation, showing that its invariant measure coincides with that of a single Langevin diffusion, and demonstrating exponential convergence to the invariant measure in a number of settings. We introduce a new noisy variant on ensemble Kalman inversion (EKI) algorithms found from the original SDE by replacing exact gradients with ensemble differences; this defines the ensemble Kalman sampler (EKS). Numerical results are presented which demonstrate its efficacy as a derivative-free approximate sampler for the Bayesian posterior arising from inverse problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Dynamical Systems},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Garbuno-Inigo2019 - Interacting Langevin Diffusions.pdf},
}
@article{Carpenter2017,
  author = {Bob Carpenter and Andrew Gelman and Matthew D. Hoffman and Daniel Lee and Ben Goodrich and Michael Betancourt and Marcus Brubaker and Jiqiang Guo and Peter Li and Allen Riddell},
  title = {Stan: a Probabilistic Programming Language},
  journal = {Journal of Statistical Software},
  volume = {76},
  number = {1},
  pages = {nil},
  year = {2017},
  doi = {10.18637/jss.v076.i01},
  url = {https://doi.org/10.18637/jss.v076.i01},
  DATE_ADDED = {Tue Jun 29 15:03:33 2021},
}
