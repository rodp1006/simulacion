#+TITLE: EST-24107: Simulación
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Simulación con cadenas de Markov~  
#+STARTUP: showall
:LATEX_PROPERTIES:
#+OPTIONS: toc:nil date:nil author:nil tasks:nil
#+LANGUAGE: sp
#+LATEX_CLASS: handout
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[sort,numbers]{natbib}
#+LATEX_HEADER: \usepackage[utf8]{inputenc} 
#+LATEX_HEADER: \usepackage[capitalize]{cleveref}
#+LATEX_HEADER: \decimalpoint
#+LATEX_HEADER:\usepackage{framed}
#+LaTeX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \definecolor{backcolour}{rgb}{.95,0.95,0.92}
#+LaTeX_HEADER: \definecolor{codegray}{rgb}{0.5,0.5,0.5}
#+LaTeX_HEADER: \definecolor{codegreen}{rgb}{0,0.6,0} 
#+LaTeX_HEADER: {}
#+LaTeX_HEADER: {\lstset{language={R},basicstyle={\ttfamily\footnotesize},frame=single,breaklines=true,fancyvrb=true,literate={"}{{\texttt{"}}}1{<-}{{$\bm\leftarrow$}}1{<<-}{{$\bm\twoheadleftarrow$}}1{~}{{$\bm\sim$}}1{<=}{{$\bm\le$}}1{>=}{{$\bm\ge$}}1{!=}{{$\bm\neq$}}1{^}{{$^{\bm\wedge}$}}1{|>}{{$\rhd$}}1,otherkeywords={!=, ~, $, \&, \%/\%, \%*\%, \%\%, <-, <<-, ::, /},extendedchars=false,commentstyle={\ttfamily \itshape\color{codegreen}},stringstyle={\color{red}}}
#+LaTeX_HEADER: {}
#+LATEX_HEADER_EXTRA: \definecolor{shadecolor}{gray}{.95}
#+LATEX_HEADER_EXTRA: \newenvironment{NOTES}{\begin{lrbox}{\mybox}\begin{minipage}{0.95\textwidth}\begin{shaded}}{\end{shaded}\end{minipage}\end{lrbox}\fbox{\usebox{\mybox}}}
#+EXPORT_FILE_NAME: ../docs/08-mcmc.pdf
:END:
#+PROPERTY: header-args:R :session mcmc :exports both :results output org :tangle ../rscripts/08-mcmc.R :mkdirp yes :dir ../ :eval never
#+EXCLUDE_TAGS: toc latex


#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Otoño, 2022 | MCMC.\\
*Objetivo*. Estudiar el método general de integración Monte Carlo vía cadenas de Markov (MCMC). La estrategia será construir poco a poco utilizando los principios básicos que lo componen. \\
*Lectura recomendada*: Capítulo 7 de citep:Dogucu2021. Sección 6 de [[citep:Sanz-Alonso2019]] y Capítulo 3 de citep:Reich2015 (avanzado). Si te interesa saber mas sobre programación orientada a objetos dentro del contexto de ~R~ puedes consultar citep:Wickham2019. 
#+END_NOTES


* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introduccion][Introduccion]]
  - [[#maldición-de-la-dimensionalidad][Maldición de la dimensionalidad]]
- [[#muestreo-por-aceptación-rechazo][Muestreo por aceptación rechazo]]
  - [[#implementación][Implementación]]
    - [[#disclaimer][Disclaimer:]]
    - [[#implementación-de-una-distribución-de-muestreo][Implementación de una distribución de muestreo.]]
  - [[#ejercicio-0][Ejercicio (0)]]
  - [[#propiedades][Propiedades]]
    - [[#lema-consistencia-de-muestreo-por-aceptación-rechazo][Lema [Consistencia de muestreo por aceptación-rechazo]:]]
- [[#qué-hemos-visto][¿Qué hemos visto?]]
- [[#muestreo-por-cadenas-de-markov][Muestreo por cadenas de Markov]]
  - [[#definición-cadena-de-markov][Definición [Cadena de Markov]:]]
- [[#generalizando][Generalizando...]]
  - [[#pseudo-código][Pseudo-código]]
  - [[#desentrañando][Desentrañando]]
  - [[#implementación][Implementación]]
  - [[#ejercicio-1][Ejercicio (1)]]
  - [[#ejercicio-2][Ejercicio (2)]]
    - [[#ejercicio][Ejercicio:]]
- [[#el-método-metropolis-hastings][El método Metropolis-Hastings]]
  - [[#ejercicio-3][Ejercicio (3)]]
  - [[#distribución-propuesta][Distribución propuesta]]
- [[#en-más-dimensiones][En más dimensiones]]
- [[#por-qué-funciona][¿Por qué funciona?]]
  - [[#definición-invarianza][Definición [Invarianza]:]]
  - [[#lema-comportamiento-asintótico-de-metropolis-hastings][Lema [Comportamiento asintótico de Metropolis-Hastings]:]]
:END:

* Introduccion


#+begin_src R :exports none :results none

  ## Setup --------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)
  ## Problemas con mi consola en Emacs
  options(pillar.subtle = FALSE)
  options(rlang_backtrace_on_error = "none")
  options(crayon.enabled = FALSE)

  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)
  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())

  #+end_src


El interés es poder resolver
\begin{align}
\pi(f) = \int_{\Theta}^{} f(\theta) \, \pi(\theta ) \,  \text{d}\theta\,. 
\end{align}

Sin embargo, ~no podemos generar~ $\theta^{(i)} \overset{\mathsf{iid}}{\sim} \pi(\theta)$ para $i = 1, \ldots, N$.

#+REVEAL: split
De lo que hemos discutido previamente nos interesa poder resolver este tipo de problemas por medio una estimación Monte Carlo
\begin{gather*}
\pi(f) = \mathbb{E}_\pi[f] = \int f(\theta) \pi(\theta) \text{d}\theta\,,\\
\pi_N^{\textsf{MC}}(f) = \frac1N \sum_{n = 1}^N f( \theta^{(n)}), \qquad \text{ donde }  \theta^{(n)} \overset{\mathsf{iid}}{\sim} \pi, \qquad \text{ con } n = 1, \ldots, N \,, \\
\pi_N^{\textsf{MC}}(f) \approx  \pi(f)\,.
\end{gather*} 

#+REVEAL: split
Por ejemplo, hemos resuelto el problema de estimar la constante $\pi$ por medio
de estimar el cociente del área de un círculo de radio 1 dentro de un cuadrado
con lados de longitud igual a 2.

#+caption: Integración Monte Carlo para aproximar $\pi$. 
#+attr_html: :width 1200 :align center
[[file:../images/dardos-montecarlo.jpeg]]

#+REVEAL: split
Para lo cual hemos discutido que el método Monte Carlo nos da un método de estimación
con tasas de convergencia de $1/\sqrt{N}$.

Aunque la dimensión del problema $\theta \in \Theta \subset \mathbb{R}^p$ no
aparece en la expresión de la varianza de nuestros estimadores. Hemos discutido
que si lo hace, de manerea implícita, en la expresión de $\mathbb{V}_\pi(f)$. Veamos un ejemplo
el cual se conoce como ~maldición de la dimensionalidad~.

** Maldición de la dimensionalidad

Consideremos de interés estimar la proporción de volumen de la hiper-esfera
contenida en un hiper-cubo unitario conforme aumenta la dimensión del problema.

#+begin_src R :exports code :results none
  distancia_euclideana <- function(u) sqrt(sum(u * u));

  experimento <- function(ndim){
    nsamples <- 1e5; 
    y <- matrix(runif(nsamples * ndim, -0.5, 0.5), nsamples, ndim);
    mean(apply(y, 1, distancia_euclideana) < 0.5)
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/curse-dimensionality.jpeg :exports results :results output graphics file
  tibble(dims = 1:10) |>
    mutate(prob = map_dbl(dims, experimento)) |>
    ggplot(aes(dims, prob)) +
    geom_point() +
    geom_line() +
    sin_lineas +
    scale_x_continuous(breaks=c(1, 3, 5, 7, 9)) +
  xlab("Número de dimensiones") +
  ylab("Volumen relativo")
#+end_src
#+caption: Evolución del volumen relativo de la hiper-esfera circunscrita dentro del hiper-cubo unitario.
#+RESULTS:
[[file:../images/curse-dimensionality.jpeg]]

#+REVEAL: split
Otro detalle interesante de altas dimensiones es la poca intuición
probabilística que tenemos de estos espacios y de lo que es una muestra típica
de una distribución.

Por ejemplo, para $X \sim \mathsf{N}(0,1)$ estamos acostumbrados a asociar la
moda como el valor de mas alta densidad. Lo cual es un error terrible en varias
dimensiones.

#+REVEAL: split
Consideremos un análisis analítico. Por ejemplo, sabemos que si $X \sim \mathsf{N}(0, I_p)$, entonces tenemos que
\begin{align}
\sum_{i = 1}^{p}X_i^2 \sim \chi^2_{p}\,.
\end{align}
Gráfiquemos el valor central de estas variables aleatorias y sus percentiles del
$2.5\%$ y $97.5\%$. Lo que observamos es que una ~muestra típica~ no se comporta
como el promedio de nuestra distirbución, /aka/ el individuo promedio no es tan
común.

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/typical-sets.jpeg :exports results :results output graphics file
  tibble(dim = 2**seq(0, 8)) |>
    mutate(.centro = sqrt(qchisq(.50, dim)),
           .lower = sqrt(qchisq(.025, dim)),
           .upper = sqrt(qchisq(.975, dim))) |>
  ggplot(aes(dim, .centro)) +
  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = .3, fill = "gray") + 
  geom_line() + geom_point() + sin_lineas +
  scale_x_log10() +
  ylab("Distancia euclideana al centro") +
  xlab("Número de dimensiones")
#+end_src
#+caption: Distancia euclideana de puntos aleatorios de una Gaussiana multivariada al centro de la distribución. Esto ilustra que aunque el centro es el comportamiento promedio, los puntos típicos de una Gaussiana se encuentran lejos. 
#+RESULTS:
[[file:../images/typical-sets.jpeg]]

#+REVEAL: split
Lo que sucede se conoce como el fenómeno de ~concentración de medida~ donde los
puntos de más alta densidad no corresponden a los puntos de mayor volumen
(probabilidad).

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/concentracion-medida.jpeg :exports results :results output graphics file
    tibble(dim = 2**seq(0, 8)) |>
      mutate(.resultados  = map(dim, function(ndim){
               x <- unlist(purrr::rerun(10000, sum(dnorm(rnorm(ndim),log = TRUE))))
               tibble(x = x) |>
                 summarise(.densidad_tip = mean(x),
                           .lower_densidad = quantile(x, .025),
                           .upper_densidad = quantile(x, .975),
                           .densidad_moda = sum(dnorm(rep(0, ndim), log = TRUE)))
             })) |>
      unnest(.resultados) |>
      ggplot(aes(dim, .densidad_tip)) +
      geom_line(aes(y = .densidad_moda), col = 'red') +
      geom_point(aes(y = .densidad_moda), col = 'red') + 
      geom_ribbon(aes(ymin = .lower_densidad, ymax = .upper_densidad), alpha = .3, fill = "gray") + 
      geom_line() + geom_point() + sin_lineas +
      scale_x_log10() +
      ylab("log-Densidad") +
      xlab("Número de dimensiones")

#+end_src
#+caption: En rojo la log-densidad de la moda de una Gaussiana multivariada. En negro la log-densidad de muestras aleatorias de una Gaussiana multivariada. Esto muestra que los elementos con mayor densidad no corresponden a vecindades de mayor volumen. 
#+RESULTS:
[[file:../images/concentracion-medida.jpeg]]

#+REVEAL: split
Esto explica por qué no queremos realizar la aproximación
\begin{align}
\pi(f) \approx f(\theta^\star)\,, \quad \text{ donde }  \quad \theta^\star = \underset{\theta \in \Theta}{\arg \max} \, \pi(\theta)\,. 
\end{align}

* Muestreo por aceptación rechazo

Podemos utilizar una versión estocástica de muestreo por importancia.

#+BEGIN_NOTES
Para muestrear de $\pi$ necesitamos utilizar una distribución sustituto (lo
mismo hicimos con muestreo por importancia). Sólo que ahora permitimos rechazar
muestras que no correspondan con las regiones de alta densidad de nuestra
distribución objetivo. El rechazo se realiza lanzando una moneda. La tasa de
éxito depende del qué tanto cubre nuestra distribución sustituto.
#+END_NOTES

#+begin_src R :exports none :results none
  ## Muestreo por aceptacion rechazo ---------------
#+end_src

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/rejection-sampling.jpeg :exports results :results output graphics file
  crea_mezcla <- function(weights){
    function(x){
      weights$w1 * dnorm(x, mean = -1.5, sd = .5) +
        weights$w2 * dnorm(x, mean = 1.5, sd = .7)
    }
  }

  objetivo <- crea_mezcla(list(w1 = .6, w2 = .4))

  tibble(x = seq(-5, 5, length.out = 100)) |>
    mutate(y = objetivo(x),
           aprox = 3.3 * dnorm(x, 0, sd = 2)) |>
    ggplot(aes(x,y)) +
    geom_area(fill = "lightblue") +
    geom_line(aes(x, aprox), lty = 2) +
    geom_ribbon(aes(ymin = y, ymax = aprox), fill = "salmon") + sin_lineas +
    sin_ejes

#+end_src
#+attr_latex: :width .55\linewidth
#+caption: Esquema de muestreo. 
#+RESULTS:
[[file:../images/rejection-sampling.jpeg]]

** Implementación
Necesitamos algunas cosas. Ser capaces de ~evaluar~ nuestra distribución
objetivo. Ser capaces de ~evaluar~ *y* ~muestrear~ de nuestra distribución de
muestreo.

#+REVEAL: split
#+caption: Distribución objetivo. 
#+begin_src R :exports code :results none
  crea_mezcla <- function(weights){
    function(x){
      weights$w1 * dnorm(x, mean = -1.5, sd = .5) +
        weights$w2 * dnorm(x, mean = 1.5, sd = .7)
    }
  }
  objetivo <- crea_mezcla(list(w1 = .6, w2 = .4))
  M        <- 3.3
#+end_src

*** /Disclaimer/:
:PROPERTIES:
:reveal_background: #00468b
:END:

El objetivo del curso *no* es enseñar programación orientada a objetos. Sin
embargo, permitirá abstraer los puntos importantes y concentrarnos en las ideas
generales y no preocuparnos por lo detalles.

*** Implementación de una distribución de muestreo.

Recordemos que lo que queremos son dos cosas: 1) generar números aleatorios y 2) evaluar la función de densidad. 

#+caption: Distribución de muestreo. 
#+begin_src R :exports code :results none
  library(R6)
  ModeloNormal <- R6Class("ProbabilityModel", list(
        mean = NA, sd = NA,
        initialize = function(mean = 0, sd = 1){ ## Inicializador
          self$mean = mean; self$sd   = sd
        },
        sample = function(n = 1){                ## Muestreador
          rnorm(n, self$mean, sd = self$sd)              
        },
        density = function(x, log = TRUE){       ## Densidad
          dnorm(x, self$mean, sd = self$sd, log = log)
        }))
#+end_src

#+BEGIN_NOTES
En muestreo con rechazo necesitamos definir una distribución de la
cual *si podamos* generar números aleatorios. El inconveniente es, además, *conocer*
qué tanto podemos inflar la densidad de nuestra propuesta para /cubrir/ la
distribución objetivo.
#+END_NOTES

#+REVEAL: split
#+caption: Algoritmo de muestreo con rechazo. 
#+begin_src R :exports code :results none
  crea_rejection_sampling <- function(objetivo, aprox, M){
    function(niter){
      muestras <- matrix(nrow = niter, ncol = 3)
      for (ii in seq(1, niter)){
        propuesta <- aprox$sample()
        p <- objetivo(propuesta)
        g <- aprox$density(propuesta, log = FALSE)
        u <- runif(1)
        if (u < p/(M * g)) {  ## Aceptamos 
          muestras[ii, 1] <- 1
        } else {              ## Rechazamos 
          muestras[ii, 1] <- 0
        }
        muestras[ii, 2] <- propuesta
        muestras[ii, 3] <- u 
      }
      colnames(muestras) <- c("accept", "value", "auxiliar")
      muestras
    }
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/muestreo-aceptacion.jpeg  :exports results :results output graphics file
  modelo.muestreo  <- ModeloNormal$new(mean = 0, sd = 2)
  muestreo_rechazo <- crea_rejection_sampling(objetivo, modelo.muestreo, M)

  muestras <- muestreo_rechazo(5000) |>
    as.tibble() |>
    mutate(density = modelo.muestreo$density(value, log = FALSE))

  g1 <- muestras |>
    ggplot(aes(value, auxiliar * modelo.muestreo$density(value, log = FALSE))) +
    geom_point(aes(color = factor(accept))) + sin_lineas + sin_ejes + sin_leyenda +
    xlab("") + ylab("") +
    ggtitle(paste("Muestras en el espacio (x,u), aceptación: ", mean(muestras$accept)))

  g2 <- muestras |>
    filter(accept == 1) |>
    ggplot(aes(value)) +
    geom_histogram() + 
    sin_lineas + sin_ejes + sin_leyenda +
    xlab("") + ylab("") +
    ggtitle("Histograma de las muestras generadas")

  g1 + g2 
#+end_src

#+RESULTS:
[[file:../images/muestreo-aceptacion.jpeg]]

** Ejercicio (0)
:PROPERTIES:
:reveal_background: #00468b
:END:

- ¿Qué pasa si $M$ es demasiado grande? Juega con el código e interpreta los resultados. 
- ¿Qué pasa si $M$ no es suficiente para cubrir la distribución objetivo? Juega con el código e interpreta los resultados.

** Propiedades

*** ~Lema~ [Consistencia de muestreo por aceptación-rechazo]:
El método de muestreo por aceptación-rechazo genera muestras $x^{(i)}$ con $i =
1, \ldots, N$ que son independientes y distribuidas acorde a la ~distribución objetivo~ $\pi(\cdot)$ utilizando una ~distribución de muestreo~ $\rho(\cdot)$.

#+REVEAL: split
/Prueba/. Usemos probabilidad condicional para medir
\begin{align}
\pi(x | \textsf{aceptar}) = \frac{\pi(\textsf{aceptar} | x) \times \rho(x)}{\pi(\textsf{aceptar})}\,.
\end{align}

* ¿Qué hemos visto?

- El método Monte Carlo se puede utilizar para aproximar integrales.
- Se puede utilizar una distribución sustituto para generar números aleatorios que nos interesan.
- Podemos lanzar monedas para /filtrar/ sólo los aleatorios que tengan altas probabilidades.
- Hemos utilizado el supuesto de independencia.

* Muestreo por cadenas de Markov

Vamos a ~relajar~ el supuesto de ~independencia~. Es decir, vamos a generar una
secuencia de números aleatorios con cierta correlación.

*** ~Definición~ [Cadena de Markov]:
Un *proceso estócastico* en tiempo discreto---es decir, una colección de
variables aleatorias $X_1, X_2, \ldots$ con probabilidades conjuntas---que
satisface la propiedad de dependencia condicional
\begin{align}
    \mathbb{P}\left( X_{n+1}  = x | X_1 = x_1, \ldots, X_n = x_n \right) = \mathbb{P}\left( X_{n+1}  = x | X_n = x_n \right)\,,
\end{align}
se llama una *cadena de Markov* en tiempo discreto.

** Ejemplo:

#+DOWNLOADED: screenshot @ 2022-02-03 12:21:07
#+caption: Problema del café. 
#+attr_html: :width 1200 :align center
[[file:images/20221031-163123_screenshot.png]]

#+REVEAL: split
El vendedor de galletas quiere satisfacer la demanda para acompañar un café. El vendedor:
- Viaja entre las islas.
- Decide si se queda o no se queda en la isla donde está. 
- Se puede mover entre islas contiguas (a través de puentes). 
- Tiene mala memoria y  pregunta el número de casas en las islas aledañas (todos los días).
- Quiere visitar todas las islas y vender galletas.
- Viaja en bicicleta. 


#+REVEAL: split
También es astuto. Sabe que en /donde haya mucha gente venderá mas/, pero también
sabe que una isla siempre lo /podría llevar a una mas grande/. Asi que a veces le
convendrá viajar a una isla pequeña. Asi que utilizará el ~principio de
aceptación rechazo~ para decidir si se moverá a la siguiente isla.

#+REVEAL: split
1. Lanza una moneda para decidir si se mueve a la izquierda o derecha.
2. Decide si se mueve de acuerdo al cociente de poblaciones.

** Pregunta

En el contexto de nuestro problema ¿qué cambiaría si tuviera conocimiento censal
del archipiélago y pudiera viajar en avión?

** Modelación del /tour/ de ventas

El vendedor se encuentra en el $t$ -ésimo día. Supongamos que va a evaluar si se
cambia a la isla de la derecha. Sea $\pi_\star$ la población de la isla propuesta y
$\pi_{t}$ la población de la isla actual. Entonces el vendedor acepta cambiar de isla
con probabilidad

$$\alpha_{\textsf{mover}}= \frac{\pi_\star}{\pi_{t}}\,.$$

#+BEGIN_NOTES
Nota que nunca dudará moverse a una isla mas grande. Por otro lado, entre mas
parecidas sean las poblaciones de las islas mas *indeciso* será de moverse. Por
definición $\alpha_{\textsf{mover}} \in (0,1)$. De hecho, podemos definir la
probabilidad de aceptar un viaje a otra isla por medio de

$$\alpha(t, \star) = \min \Bigg\{ 1, \frac{\pi_\star}{\pi_{t}}\Bigg\},$$

pues incluye los dos casos. 
#+END_NOTES

#+REVEAL: split
#+begin_src R :exports none :results none
  ## Caminata entre islas --------------------------
  set.seed(1087)
#+end_src

#+caption: Mecanismo de cambio o permanencia desde la isla $i$. 
#+begin_src R :exports code :results none
  islas <- tibble(islas = 1:7, pob = c(1,2,3,4,5,4,3))
  camina_isla <- function(i){ # i: isla actual
    u_izq <- runif(1) # Lanzamos volado para ver si nos vamos izq o der. 
    v <- ifelse(u_izq < 0.5, i - 1, i + 1)  # Pedimos índice isla vecina. 
    if (v < 1 | v > 7) { # si estas en los extremos y el volado indica salir
      return(i)
    }
    u_cambio <- runif(1) # Moneda de aceptacion de cambio
    p_cambio = min(islas$pob[v]/islas$pob[i], 1)
    if (u_cambio < p_cambio) {
      return(v) # isla destino
    }
    else {
      return(i) # me quedo en la misma isla
    }
  }
#+end_src

#+REVEAL: split
#+begin_src R :exports none :results none
  pasos <- 100000; iteraciones <- numeric(pasos)
  iteraciones[1] <- sample(1:7, 1) # isla inicial
  for (j in 2:pasos) {
      iteraciones[j] <- camina_isla(iteraciones[j - 1])
  }
  caminata <- tibble(paso = 1:pasos, isla = iteraciones)
#+end_src

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/caminata-cafe.jpeg :exports results :results output graphics file
  plot_caminata <- ggplot(caminata[1:500, ], aes(x = paso, y = isla)) +
    geom_point(size = 0.8) +
    geom_path(alpha = 0.5) +
    labs(title = "Caminata aleatoria") +
    scale_x_continuous(trans = "log10", "Tiempo", breaks = c(1, 2, 5, 20, 100, 500)) +
    scale_y_continuous( expression(theta)) + sin_lineas
  plot_dist <- ggplot(caminata, aes(x = isla)) +
    geom_bar(fill = "darkgray", aes(y = (..count..)/sum(..count..))) +
    geom_bar(data = islas |>  mutate(prop = pob/sum(pob)),
             aes(x = islas, y = prop), fill = "steelblue", alpha = .3, stat = "identity") + 
    scale_x_continuous(expression(theta), breaks = 1:10) +
    ylim(0,.5) + 
    labs(title = "Distribución objetivo (Histograma)", 
         y = expression(hat(pi)(theta))) + sin_lineas + coord_flip()
  plot_caminata + plot_dist
#+end_src
#+caption: Caminata aleatoria en un archipiélago de 7 islas. 
#+RESULTS:
[[file:../images/caminata-cafe.jpeg]]

#+begin_src R :exports none :results none :eval never :tangle no
  ## Animación histograma -----------------------------------
  library(gganimate)
  res <- caminata |>
    mutate(tiempo = cut(paso, breaks = seq(0, n(), by = 10))) |>
    group_by(isla, tiempo) |>
    count() |>
    ungroup() |>
    complete(tiempo, nesting(isla), fill = list(n = 0)) |>
    group_by(isla) |>
    mutate(count = cumsum(n)) |>
    group_by(tiempo) |>
    mutate(prop = count/sum(count)) |>
    arrange(tiempo, isla) |>
    ungroup()

  anim <- res |>
    mutate(tiempo = as.numeric(tiempo)) |>
    filter(tiempo <= 1500) |>
    ggplot(aes(x = isla, y = prop)) +
    geom_bar(fill = "darkgray", stat = "identity") +
    coord_flip() + sin_lineas +
    geom_bar(data = islas |>  mutate(prop = pob/sum(pob)),
             aes(x = islas, y = prop), fill = "steelblue", alpha = .3, stat = "identity") + 
    scale_x_continuous(expression(theta), breaks = 1:10) +
    transition_states(tiempo, transition_length = 2, state_length = 1) +
    ease_aes("exponential-out")

  animate(anim, renderer = ffmpeg_renderer(), height = 300, width = 900)

  anim_save("./images/islas-histograma.mp4")

#+end_src

** Conclusiones

- La estrategia del vendedor le permitirá, en el ~largo plazo~,  visitar todas las islas.
- La proporción de tiempo que pasa en cada isla$^\dagger$ corresponde a la población relativa.
- Al principio, aún no representa dicha proporción.

* Generalizando... 

Supongamos que tenemos un modelo
\begin{gather}
Y| \mu \sim \mathsf{N}(\mu, 0.75^2)\,,\\
\mu \sim \mathsf{N}(0,1^2)\,.
\end{gather}

Bajo la observación $y = 6.25$ la distribución que nos interesa es
\begin{gather}
\mu | y \sim \mathsf{N}(4, 0.6^2)\,.
\end{gather}

#+REVEAL: split
~Vamos a suponer~ que *no* sabemos muestrear de una Normal. Asi que usaremos una
estrategia parecida que con el vendedor de galletas. La estrategia será:
1. Generar una propuesta $\mu_\star$ para cambiarnos de nuestro valor actual $\mu_t$.
2. Decidir si nos movemos utilizando un cociente que tome en cuenta los pesos relativos.

** Pseudo-código 
- Vamos a proponer una ``moneda'' para lanzar la *dirección* de movimiento. Esto
  lo haremos con
  \begin{align}
  \mu_\star | \mu_t \sim \mathsf{Uniforme}( \mu_t - \omega, \mu_t + \omega)\,.
  \end{align}

#+REVEAL: split
- Vamos a decidir si nos movemos de acuerdo a los pesos relativos
  \begin{align}
  \alpha(\mu_t, \mu_\star)  = \min \left\lbrace1 , \frac{\pi(\mu_\star)}{\pi(\mu_t)} \right\rbrace\,.
  \end{align}
  
** Desentrañando

Escribamos el cociente en términos de la densidad de la distribución posterior y simplifiquemos. ¿Qué observas? 

** Implementación

Veamos cómo implementarlo. Vamos a suponer una distribución de muestreo con un intervalo de longitud 2. Es decir,  $\omega = 1$. 

#+begin_src R :exports none :results none
  ## Caminata en espacio continuo ------------------------
#+end_src

#+REVEAL: split
#+caption: Modelo de muestreo uniforme. 
#+begin_src R :exports code :results none
  ModeloUniforme <- R6Class("ProbabilityModel", list(
       a = NA, b = NA, 
       initialize = function(a = 0, b = 1){
         self$a = a; self$b = b
       }, 
       sample = function(n = 1){
         runif(n, self$a, self$b)              
       },
       density = function(x, log = TRUE){
         dunif(x, self$a, self$b, log = log)
       }))
#+end_src

#+REVEAL: split
#+caption: Nuestra segunda cadena de Markov. 
#+begin_src R :exports code :results none
  crea_cadena_markov <- function(objetivo, muestreo){
    function(niter){
      muestras <- matrix(nrow = niter, ncol = 2)
      ## Empezamos en algun lugar
      estado   <- muestreo$sample()
      muestras[1,2] <- estado
      muestras[1,1] <- 1
      for (ii in 2:niter){
        ## Generamos un candidato
        propuesta   <- estado + muestreo$sample()
        p_propuesta <- objetivo$density(propuesta, log = FALSE)
        p_estado    <- objetivo$density(estado, log = FALSE)
        ## Evaluamos probabilidad de aceptar
        if (runif(1) < p_propuesta/p_estado) {
          muestras[ii, 1] <- 1 ## Aceptamos
          muestras[ii, 2] <- propuesta
        } else {
          muestras[ii, 1] <- 0 ## Rechazamos
          muestras[ii, 2] <- estado
        }
        estado <- muestras[ii, 2]
      }
      colnames(muestras) <- c("accept", "value")
      muestras
    }
  }
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  objetivo <- ModeloNormal$new(mean = 4, sd = .6)
  muestreo <- ModeloUniforme$new(a = -1, b = 1)

  mcmc <- crea_cadena_markov(objetivo, muestreo)
  muestras <- mcmc(5000)
#+end_src

#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/primer-mcmc.jpeg :exports results :results output graphics file
  g1 <- muestras |>
    as.tibble() |>
    mutate(iter = 1:n()) |>
    ggplot(aes(iter, value)) +
    geom_line() + sin_lineas + 
    ggtitle(paste("Trayectoria, aceptación: ", mean(muestras[,1])))

  g2 <- muestras |>
    as.tibble() |>
    ggplot(aes(value)) +
    geom_histogram(aes(y = ..density..)) +
    stat_function(fun = objetivo$density,
                  args = list(log = FALSE),
                  color = "salmon",
                  size = 2) + sin_lineas + 
    ggtitle("Histograma")

  g1 + g2
#+end_src
#+caption: Nuestra segunda cadena de Markov. 
#+RESULTS:
[[file:../images/primer-mcmc.jpeg]]

** Ejercicio (1)
:PROPERTIES:
:reveal_background: #00468b
:END:

Sin modificar el número de iteraciones, considera cambiar la dispersión de la distribución de muestreo.
- ¿Qué observas si $\omega = 0.01$?
- ¿Qué observas si $\omega = 100$?

** Ejercicio (2)
:PROPERTIES:
:reveal_background: #00468b
:END:

Regresa a nuestro ejemplo conjugado Beta-Binomial. Considera una previa $\theta \sim \mathsf{Beta}(2,3)$ y una verosimilitud $Y|\theta \sim \mathsf{Binomial}(2, \theta)$. Escribe la distribución posterior asumiendo $Y = k$. 

#+REVEAL: split
Para este caso tenemos un ligero inconveniente. El soporte para $\theta$ es el intervalo cerrado $[0,1]$ y utilizar una propuesta como en el caso anterior nos podría colocar (casi seguramente) fuera del intervalo. Así que lo que haremos será un pequeña modificación a cómo generamos nuestra propuesta y cómo evaluamos la probabilidad de aceptar dicha propuesta.

#+REVEAL: split
- Vamos a generar propuestas de la siguiente manera
  \begin{align}
  \theta_\star | \theta_t \sim \mathsf{Beta}(\alpha, \beta)\,.
  \end{align}
- Vamos a calcular la probabilidad de aceptar dicho movimiento a través de
  \begin{align}
  \alpha(\theta_t, \theta_\star) = \min \left\lbrace 1,  \frac{\pi(\theta_\star|y)}{\pi(\theta_t|y)} \cdot \frac{g(\theta_t)}{g(\theta_\star)}\right\rbrace\,,
  \end{align}
  donde $g$ denota la densidad de la distribución de muestreo definida arriba.

  



*** Ejercicio:
:PROPERTIES:
:reveal_background: #00468b
:END:
Modifica el código de clase para implementar este muestreador. Utiliza distintas
configuraciones de $a,b$ para la distribución de propuesta. Compara con muestras
exactas del modelo posterior bajo la observación $Y = 1$.

* El método Metropolis-Hastings 

La forma más general que tenemos para generar una cadena de muestras es el método de Metropolis-Hastings.
#+REVEAL: split

- Generamos propuestas en cada iteración por medio de 
  \begin{align}
  \theta_\star | \theta_t \sim q( \theta_\star | \theta_t )\,.
  \end{align}
- Calculamos la probabilidad de aceptar la propuesta como 
  \begin{align}
  \alpha(\theta_t, \theta_\star) = \min \left\lbrace 1,  \frac{\pi(\theta_\star)}{\pi(\theta_t)} \cdot \frac{q(\theta_t|\theta_\star)}{q(\theta_\star|\theta_t)}\right\rbrace\,,
  \end{align}
  donde la notación hace énfasis en que este mecanismo puede generar muestras de
  la distribución $\pi$ utilizando un generador $q$.

** Ejercicio (3)
:PROPERTIES:
:reveal_background: #00468b
:END:

- Repasemos ~los métodos anteriores~.
- ¿Qué pasa si desconocemos la constante de normalización de la distribución objetivo?

** Distribución propuesta

El /arte/ está en proponer una distribución de muestreo eficiente. Como ya hemos
discutido, si no está bien calibrada podríamos tener un comportamiento no
deseado. Supongamos que queremos muestrear de una $\mathsf{Gamma}(20,
100)$. Para esto veamos tres configuraciones de la distribución de muestreo que será
$\mathsf{N}(\theta_t, \sigma^2)$. 

#+begin_src R :exports none :results none
  ## Implementacion Metropolis Hastings -----------------------
  ModeloGamma <-
    R6Class("ProbabilityModel",
            list(
              shape = NA,
              rate  = NA, 
              initialize = function(a = 0, b = 1){
                self$shape = a
                self$rate  = b
              }, 
              sample = function(n = 1){
                rgamma(n, shape = self$shape, rate = self$rate)              
              },
              density = function(x, log = TRUE){
                dgamma(x, shape = self$shape, rate = self$rate, log = log)
              }           
            ))
#+end_src

#+begin_src R :exports none :results none
  ### Muestreador Metropolis-Hastings -------------------------
  crea_metropolis_hastings <- function(objetivo, muestreo){
    ## Este muestreador aprovecha la simetría de la propuesta 
    function(niter){
      ## Empezamos en algun lugar
      estado <- muestreo$sample()
      ndim <- length(estado) 
      muestras <- matrix(nrow = niter, ncol = ndim + 1)      
      muestras[1,2:(ndim+1)] <- estado
      muestras[1,1] <- 1
      for (ii in 2:niter){
        propuesta   <- estado + muestreo$sample()
        log_pi_propuesta <- objetivo$density(propuesta)
        log_pi_estado    <- objetivo$density(estado)
        log_alpha <- log_pi_propuesta - log_pi_estado

        if (log(runif(1)) < log_alpha) {
          muestras[ii, 1] <- 1 ## Aceptamos
          muestras[ii, 2:(ndim+1)] <- propuesta
        } else {
          muestras[ii, 1] <- 0 ## Rechazamos
          muestras[ii, 2:(ndim+1)] <- estado
        }
        estado <- muestras[ii, 2:(ndim+1)]
      }
      if (ndim == 1) {colnames(muestras) <- c("accept", "value")}
      muestras
    }
  }
#+end_src


#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/mh-pasochico.jpeg :exports results :results output graphics file
  set.seed(108727)
  objetivo <- ModeloGamma$new(a = 20, b = 100)
  muestreo <- ModeloNormal$new(sd = 0.001)
  mcmc_chico <- crea_metropolis_hastings(objetivo, muestreo)

  g1 <- mcmc_chico(3000) |>
    as.tibble() |>
    mutate(t = 1:n()) |>
    ggplot(aes(t, value)) +
    geom_line() + sin_lineas + ylab(expression(theta)) +
    ylim(0, 0.5)

  g2 <- tibble(x = rgamma(10000, 20, 100)) |>
    ggplot(aes(y = x, x = "")) +
    geom_violin() +
    ylab("") + sin_lineas +
    ylim(0, 0.5)

  g1 + g2 + plot_layout(widths = c(5, 1))
#+end_src
#+caption: Metropolis-Hastings en acción con un tamaño de paso muy pequeño. 
#+RESULTS:
[[file:../images/mh-pasochico.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/mh-pasogrande.jpeg :exports results :results output graphics file
  set.seed(108727)
  muestreo <- ModeloNormal$new(sd = 20)
  mcmc_grande <- crea_metropolis_hastings(objetivo, muestreo)

  g1 <- mcmc_grande(3000) |>
    as.tibble() |>
    mutate(t = 1:n()) |>
    ggplot(aes(t, value)) +
    geom_line() + sin_lineas + ylab(expression(theta)) +
    ylim(0, 0.5)

  g2 <- tibble(x = rgamma(10000, 20, 100)) |>
    ggplot(aes(y = x, x = "")) +
    geom_violin() +
    ylab("") + sin_lineas +
    ylim(0, 0.5)

  g1 + g2 + plot_layout(widths = c(5, 1))
#+end_src
#+caption: Metropolis-Hastings en acción con un tamaño de paso muy grande. 
#+RESULTS:
[[file:../images/mh-pasogrande.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/mh-pasojusto.jpeg :exports results :results output graphics file
  set.seed(108727)
  muestreo <- ModeloNormal$new(sd = 0.1)
  mcmc_justo <- crea_metropolis_hastings(objetivo, muestreo)

  g1 <- mcmc_justo(3000) |>
    as.tibble() |>
    mutate(t = 1:n()) |>
    ggplot(aes(t, value)) +
    geom_line() + sin_lineas + ylab(expression(theta)) +
    ylim(0, 0.5)

  g2 <- tibble(x = rgamma(10000, 20, 100)) |>
    ggplot(aes(y = x, x = "")) +
    geom_violin() +
    ylab("") + sin_lineas +
    ylim(0, 0.5)

  g1 + g2 + plot_layout(widths = c(5, 1))
#+end_src
#+caption: Metropolis-Hastings en acción con un tamaño de paso /justo/. 
#+RESULTS:
[[file:../images/mh-pasojusto.jpeg]]

#+REVEAL: split
#+begin_src R :exports results :results org
  tibble(configuracion = c("Paso chico", "Paso grande", "Paso justo"), 
         cadena   = c(mcmc_chico, mcmc_grande, mcmc_justo)) |>
    mutate(muestras = map(cadena, function(x) {
      set.seed(108727)
      x(3000) |>
        as.tibble()
    })) |>
    unnest(muestras) |>
    group_by(configuracion) |>
    summarise(media = mean(value),
              tasa.aceptacion = mean(accept)) |>
    rbind(tibble(configuracion = "Teorica",
                 media = objetivo$shape/objetivo$rate,
                 tasa.aceptacion = NA)) |>
    as.data.frame()
#+end_src

#+RESULTS:
#+begin_src org
  configuracion media tasa.aceptacion
1    Paso chico 0.086          0.9440
2   Paso grande 0.309          0.0067
3    Paso justo 0.197          0.4633
4       Teorica 0.200              NA
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/mh-largoplazo.jpeg :exports results :results output graphics file :eval never
  set.seed(108727)

  g1 <- mcmc_chico(1000000) |>
    as.tibble() |>
    mutate(t = 1:n()) |>
    ggplot(aes(t, value)) +
    geom_line() + sin_lineas + ylab(expression(theta)) +
    ylim(0, 0.5)

  g2 <- tibble(x = rgamma(10000, 20, 100)) |>
    ggplot(aes(y = x, x = "")) +
    geom_violin() +
    ylab("") + sin_lineas +
    ylim(0, 0.5)

  g1 + g2 + plot_layout(widths = c(5, 1))
#+end_src
#+caption: Metropolis-Hastings en acción con un tamaño de paso /pequeño/ y un periodo suficientemente amplio. 
#+RESULTS:
[[file:../images/mh-largoplazo.jpeg]]

* En más dimensiones

Consideremos la siguiente distribución objetivo
\begin{align}
\theta \sim \mathsf{N}(\textsf{m}, \textsf{S}), \qquad \textsf{m} = (1,2)^\top, \qquad \mathsf{S} = \begin{pmatrix}1 & .75\\.75 &1 \end{pmatrix}\,,
\end{align}
y utilicemos el modelo de muestreo 
\begin{align}
\theta \sim \mathsf{N}(\mathsf{0}, \mathsf{\Sigma}), \qquad \mathsf{0} \in \mathbb{R}^2, \qquad \mathsf{\Sigma} =\sigma^2 \cdot \begin{pmatrix}1 & 0\\0 &1 \end{pmatrix}\,.
\end{align}

#+begin_src R :exports none :results none
  ## En mas dimensiones -------------------------------
#+end_src
#+REVEAL: split
#+caption: Modelo de muestreo multivariado.
#+begin_src R :exports code :results none
  library(mvtnorm)
  ModeloNormalMultivariado <- R6Class("ProbabilityModel", list(
        mean = NA,
        cov  = NA, 
        initialize = function(mu = 0, sigma = 1){
          self$mean = mu; self$cov  = sigma |> as.matrix()
        }, 
        sample = function(n = 1){
          rmvnorm(n, mean = self$mean, sigma = self$cov)              
        },
        density = function(x, log = TRUE){
          dmvnorm(x, self$mean, self$cov, log = log)              
        }))
#+end_src

#+begin_src R :exports none :results none
  mu <- c(1, 2)
  Sigma <- matrix(c(1, .75, .75, 1), nrow = 2)
  objetivo <- ModeloNormalMultivariado$new(mu, Sigma)


  genera_experimento <- function(sigma){
    muestreo <- ModeloNormalMultivariado$new(c(0,0),
                                             sigma * diag(c(1,1)))
    set.seed(10)
    mcmc_multi <- crea_metropolis_hastings(objetivo, muestreo)
    mcmc_multi(50) |>
      as.tibble()
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/multinormal-propuestas-mh.jpeg :exports results :results output graphics file
  set.seed(108727)
  ## Para dibujar las curvas de nivel - distribucion objetivo 
  plot.grid <- expand_grid(x = seq(-2,5, by = 7/99), y = seq(-1,5, by = 6/99))
  plot.grid <- plot.grid %>% 
    mutate(density.target = dmvnorm(plot.grid, mean = mu, sigma = Sigma))
  plot.breaks.target <- plot.grid %>% 
    summarise(breaks = quantile(density.target, probs = c(.67, .90, .99, 1))) %>% 
    pull(breaks)

  ## Caminatas aleatorias 
  muestras.normal <- tibble(sigma = c(.1, .75, 2.33/sqrt(2), 5)) |>
     mutate(muestras = map(sigma, genera_experimento)) |>
     unnest(muestras)

  ## Para dibujar las curvas de nivel - distribucion propuesta
  contours.proposal <- muestras.normal |>
    filter(sigma == 2.33/sqrt(2)) |>
    slice(1,3,7) |> mutate(id = 1:3) |>
    nest(location = c(V2, V3)) |>
    mutate(density.proposal = map(location,
           function(x){
             dmvnorm(plot.grid |> select(x,y),
                     mean = as.matrix(x),
                     sigma = 1.65 * diag(c(1,1)))
           }),
           coords = list(plot.grid |> select(x,y))) |>
    mutate(breaks.proposal = map(density.proposal, quantile, probs = c(.67,.90,.99)))

  contours.proposal |>
    unnest(location, density.proposal, coords) |> 
    ggplot(aes(x, y, z = density.proposal)) +
    geom_contour_filled(bins = 4) + scale_fill_brewer(palette = "Purples") + 
    geom_point(data = contours.proposal |> unnest(location),
               aes(V2, V3), shape = 19, size = 10) +
    geom_contour(data = plot.grid, aes(x,y,z = density.target),
                 breaks = plot.breaks.target, color = "black") +
    xlab(expression(x[1])) + ylab(expression(x[2])) + 
    facet_wrap(~id) + sin_lineas + coord_equal() + sin_leyenda
#+end_src
#+caption: Propuestas Gaussianas (morado) contra densidad objetivo (línea sólida). Tres primeras iteraciones.
#+RESULTS:
[[file:../images/multinormal-propuestas-mh.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/multinormal-aceptacion-mh.jpeg :exports results :results output graphics file
  contours.proposal |>
    mutate(denominator = map(location, objetivo$density),
           numerator   = map(coords  , objetivo$density)) |>
    unnest(numerator, denominator) |>
    mutate(metropolis.hastings = ifelse(exp(numerator-denominator) < 1,
                                        exp(numerator-denominator), 1.00),
           contours.proposal |> unnest(coords) |> select(x,y),
           contours.proposal |> unnest(density.proposal),
           alpha = metropolis.hastings * density.proposal) |>
    ggplot(aes(x, y, z = log(metropolis.hastings + 1))) +
    geom_contour_filled(bins = 7) +
    scale_fill_brewer(palette = "Purples", direction = 1) +
    facet_wrap(~id) + sin_lineas + coord_equal() + sin_leyenda + 
    geom_point(data = contours.proposal |> unnest(location),
               aes(V2, V3), inherit.aes = FALSE, shape = 19, size = 10) +
    xlab(expression(x[1])) + ylab(expression(x[2])) 
#+end_src
#+caption: Probabilidad de aceptación de la propuesta de transición.
#+RESULTS:
[[file:../images/multinormal-aceptacion-mh.jpeg]]


#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/multinormal-transicion-mh.jpeg :exports results :results output graphics file
  contours.proposal |>
    mutate(denominator = map(location, objetivo$density),
           numerator   = map(coords  , objetivo$density)) |>
    unnest(numerator, denominator) |>
    mutate(metropolis.hastings = ifelse(exp(numerator-denominator) < 1,
                                        exp(numerator-denominator), 1.00),
           contours.proposal |> unnest(coords) |> select(x,y),
           contours.proposal |> unnest(density.proposal),
           alpha = metropolis.hastings * density.proposal) |>
    ggplot(aes(x, y, z = alpha)) +
    geom_contour_filled(bins = 5) +
    scale_fill_brewer(palette = "Purples") +
    facet_wrap(~id) + sin_lineas + coord_equal() + sin_leyenda + 
    geom_point(data = contours.proposal |> unnest(location),
               aes(V2, V3), inherit.aes = FALSE, shape = 19, size = 10) +
    xlab(expression(x[1])) + ylab(expression(x[2])) 
#+end_src
#+caption: Probabilidad de transición (morado) = probabilidad de proponer un nuevo estado multiplicada por la probabilidad de aceptar dicha transición. 
#+RESULTS:
[[file:../images/multinormal-transicion-mh.jpeg]]

#+begin_src R :exports none :results none :eval never :tangle no
  ##
  muestreo <- ModeloNormalMultivariado$new(c(0,0), 1.65 * diag(c(1,1)))
  set.seed(10)
  mcmc_multi <- crea_metropolis_hastings(objetivo, muestreo)

  anim <- mcmc_multi(5000) |>
    as.tibble() |>
    mutate(tiempo = seq(1,5000)) |>
    ggplot(aes(x = V2, y = V3)) +
    geom_contour_filled(data = plot.grid, aes(x,y,z = density.target),
                        breaks = plot.breaks.target, inherit.aes = FALSE) +
    scale_fill_brewer(palette = "Reds") + 
    ## geom_path(alpha = .3) +
    geom_point() + 
    xlab(expression(x[1])) + ylab(expression(x[2])) + 
    sin_lineas + coord_equal() + sin_leyenda +
    transition_reveal(tiempo) +
    shadow_trail(alpha = .3, distance = 0.01) +
    ## shadow_mark(past=TRUE, future = TRUE) + 
    ease_aes("exponential-out")

  animate(anim, renderer = ffmpeg_renderer(), height = 300, width = 300)

  anim_save("./images/caminata-aleatoria.mp4")
#+end_src



#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/multinormal.jpeg :exports results :results output graphics file
  ## Caminatas aleatorias 
  muestras.normal |>
    ggplot(aes(x = V2, y = V3)) +
    geom_contour_filled(data = plot.grid, aes(x,y,z = density.target),
                 breaks = plot.breaks.target) +
    scale_fill_brewer(palette = "Reds") + 
    geom_path() + geom_point() + 
    facet_wrap(~round(sigma,2), nrow = 1) + 
    xlab(expression(x[1])) + ylab(expression(x[2])) + 
    sin_lineas + coord_equal() + sin_leyenda
#+end_src
#+caption: Caminata aleatoria utilizando Metropolis-Hastings para $\theta\in \mathbb{R}^2$. 
#+RESULTS:
[[file:../images/multinormal.jpeg]]



* ¿Por qué funciona?

Ya vimos cómo funciona y describimos una versión suficientemente
robusta. Ahora estudiaremos el por qué esa manera de operar las transiciones nos
lleva a tener un mecanismo que genera muestras de la distribución (en el largo
plazo).

#+REVEAL: split
Para esto tenemos que preguntarnos sobre las probabilidades de transición entre
dos estados. Es decir, la probabilidad de movernos al estado $\theta_\star$
condicional en estar en $\theta$. Lo denotamos por
\begin{align}
\mathbb{P}\left(  \theta_{t + 1} = \theta_\star | \theta_t = \theta\right)\,.
\end{align}

#+REVEAL: split
Si el algoritmo es capaz de mantener un balance entre las probabilidades
condicionales entre dos estados de acuerdo a su frecuencia relativa, entonces el
algoritmo será capaz de preservar las frecuencias.

#+REVEAL: split
En palabras (bueno...), buscamos que
\begin{align}
\frac{\mathbb{P}\left(  \theta_{t + 1} = \theta_\star | \theta_t = \theta\right)}{\mathbb{P}\left(  \theta_{t + 1} = \theta | \theta_t = \theta_\star\right)} = \frac{\pi(\theta_\star)}{\pi(\theta)}\,,
\end{align}
donde $\pi(\cdot)$ denota la probabilidad objetivo.

#+REVEAL: split
Sólo nos falta calcular la probabilidad de transición. Esto lo logramos con dos
pasos: 1) generar la propuesta y 2) aceptar o rechazar la propuesta. Por lo tanto
\begin{align}
\mathbb{P}\left(  \theta_{t + 1} = \theta_\star | \theta_t = \theta\right) = q(\theta_\star | \theta ) \cdot   \alpha(\theta, \theta_\star) =  q(\theta_\star | \theta ) \cdot \min \left\lbrace 1,  \frac{\pi(\theta_\star)}{\pi(\theta)} \cdot \frac{q(\theta|\theta_\star)}{q(\theta_\star|\theta)}\right\rbrace\,. 
\end{align}

*** ~Definición~ [Invarianza]: 
Decimos que la distribución $\pi$ es ~invariante~ ante un mecanismo de transición
Markoviana ($p(u, v)$) si satisface que
\begin{align}
\int \pi(u)\, p(u, v) \text{d}u = \pi(v) \,.
\end{align}

#+BEGIN_NOTES
Lo que aprendemos de esto es que si tenemos un mecanismo de transición
Markoviana que satisface las ecuaciones de balance entonces se mantendrá el
comportamiento aleatorio de la distribución objetivo. Lo importante es que la
transición preserva la distribución objetivo.
#+END_NOTES


*** ~Lema~ [Comportamiento asintótico de Metropolis-Hastings]:
El mecanismo de MH descrito anteriormente tiene como distribución límite
$\pi(\cdot)$.

#+BEGIN_NOTES
Lo que aprendemos de esto es que en particular MH preserva las ecuaciones de
balance. Por lo tanto, si la cadena empieza en la distribución que nos interesa,
entonces se mantendrá en ese comportamiento. Estudiar formalmente las
condiciones y la tasa de convergencia para llegar a esa distribución escapa a
los intereses del curso y se puede encontrar un tratamiento mas cuidadoso de
esto en citep:Meyn1993. Sin embargo, podemos entenderlo bajo el argumento que MH
busca las zonas de alta densidad. Tal como el vendedor ambulante prefería de
manera consistente las islas mas grandes.
#+END_NOTES

bibliographystyle:abbrvnat
bibliography:references.bib
